{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Explanation\n",
    "The goal of this project was to determine if a machine learning algorithm could correctly be trained to identify people that likely commited fraud, considered \"Persons of Interest\" or \"POI\" given some information about them. For this project the data sources were generated by Katie Malone. T\n",
    "\n",
    "The dataset had numerous columns but even without looking at the data it can logically be surmised that the people who commited fraud likely used it for financial gain and that studying the financial data could give a good guess as to who was aware of or commited fraduluent activity.  The dataset contained 148 rows, 21 columns and 18 were people of interest. An initial exploration showed two outliers, Total and The Travel Agency. Both were removed from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature Selection was done through exploratory analysis, hand selecting variables, and feature engineering. Boxplots of all the features were compared between the POI and the Non POI groups. From the boxplots it was judged that there was a distinct difference in the distributions of the salaries between the two groups.\n",
    "\n",
    "When studying the missing values it became apparent that the proporition of missing features for the POIs was different than the non poi. For instance in the Other column the POIs had no missing values, whereas 43% of the Non POIs were missing values.\n",
    "\n",
    "The Other, Expenses, and Bonus features were recoded into booleans, with 1 indicating a value, and 0 indicating no value. These three features along with the Salary column were selected for use in the model. In the final model Principal Component Analysis was then performed to reduce the four features into two. Lastly all features were scaled using a Standard Scaler before the model was fitted.\n",
    "\n",
    "In initial tests attempts were made to use automated feature selection algorithms such as KBest and SelectPercentile but those were ultimately not used in the final predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "A number of algorithms were spot checked at the start of modeling. In no particular order here were the ones tested\n",
    "\n",
    "* Random Forest\n",
    "* Decision Tree\n",
    "* Gaussian Bayes\n",
    "* K Means\n",
    "* SVC with rbf kernel\n",
    "* SVC with linear kernel\n",
    "* LinearSVC (A different implementation in sklearn)\n",
    "* Adaboost\n",
    "\n",
    "All models were tested by splitting the dataset into test and training methods. Then a GridSearchCV method was used to fit multiple parameters combinations with the F1 score as the objective. These initial tests showed the most promise with the LinearSVC and K means algorithm. The other models either had a 0 recall or precision, or had poor scores in both.\n",
    "\n",
    "A choice was made to use the LinearSVC algorith, especially after reading the SKlearn documentation.\n",
    "\n",
    "#### Final Model Scores\n",
    "\n",
    "**Recall:** .33\n",
    "\n",
    "**Precision:**.35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "Turning the parameters of the model essentially means tweaking the way the model classifies data into predictions. The models were initially tuned by testing various parameters specific to each model by using the GridSearchCV function of Sklearn.\n",
    "\n",
    "After choosing the LinearSVC model the weighting parameters were turned by hand. The model was tested using StratifiedShuffleSplits with 1000 iterations. At the conclusion of each test the recall and precision scores were evaluated, the parameters were readjusted accordingly, and the model was retested. This was performed until the recall and precision were both above .3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "Validation is the act of checking the model performance against a list of known results. The classic mistake is to fit and validate the model against the same dataset, thereby over predicting the model accuracy and fit. The strategy used in the analysis was two fold. For the initial fit the various models were validated using a holdout test dataset. For the final model the entire dataset was split using a Kfold split that was shuffled over a 1000 iterations to compile aggregate metrics for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "For this project precision and recall were used to determine the final efficiacy of the effort. Precision for the model is the Positive Predictions that were also POIs over the number of positive predictions. Recall was the number of Positive Predictions that were POIs over he number of positive predictions. For a crime case such as this a high recall would be desirable as it means we are correctly flagging all fraudulent inviduals for investigation, even if the overall sensitivity of the model is low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful References\n",
    "* http://scikit-learn.org/stable/auto_examples/grid_search_digits.html\n",
    "* http://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
