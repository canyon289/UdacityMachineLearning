{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Explanation\n",
    "The goal of this project was to determine if a machine learning algorithm could correctly be trained to identify people that likely commited fraud, considered \"Persons of Interest\" or \"POI\" given some information about them. For this project the data sources were generated by Katie Malone.\n",
    "\n",
    "The dataset had numerous columns, it can logically be surmised that the people who commited fraud likely used it for financial gain and that studying the financial data could give a good guess as to who was aware of or commited fraduluent activity. The data also included features from the The dataset contained 148 rows, 21 columns and 18 were people of interest. This was a fairly imbalanced dataset with only 12% of rows being a POI. An initial exploration showed two outliers, Total and The Travel Agency. Both were removed from the dataset. Finally the dataset showed numerous missing values. When inspecting the source pdf it became clear these indicated a zero value. The exception was the email_address column, however this column was recoded and eventually removed as discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature Selection was done through exploratory analysis, hand selecting variables, and feature engineering. Boxplots of all the features were compared between the POI and the Non POI groups. From the boxplots it was judged that there was a distinct difference in the distributions of the salaries between the two groups.\n",
    "\n",
    "When studying the missing values it became apparent that the proporition of missing features for the POIs was different than the non POI. For instance in the Other column the POIs had no missing values, whereas 43% of the Non POIs were missing values.The Other, Expenses, and Bonus features were recoded into booleans, with 1 indicating a value, and 0 indicating no value. These three features along with the Salary column were selected for use in the model. In the final model Principal Component Analysis was then performed to reduce the four features into two. Lastly all features were scaled using a Standard Scaler before the model was fitted.\n",
    "\n",
    "In initial tests attempts were made to use automated feature selection algorithms such as KBest and SelectPercentile but those were ultimately not used in the final predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "A number of algorithms were spot checked at the start of modeling. In no particular order here were the ones tested\n",
    "\n",
    "* Random Forest\n",
    "* Decision Tree\n",
    "* Gaussian Bayes\n",
    "* K Means\n",
    "* SVC with rbf kernel\n",
    "* SVC with linear kernel\n",
    "* LinearSVC (A different implementation in sklearn)\n",
    "* Adaboost\n",
    "* Logistic Regression\n",
    "\n",
    "All models were tested by splitting the dataset into test and training methods. Then a GridSearchCV method was used to fit multiple parameters combinations with the F1 score as the objective. These initial tests showed the most promise with the LinearSVC, K means and Logistic Regression models. The other models either had a 0 recall or precision, or had poorer scores for both measures.\n",
    "\n",
    "### Final Model Scores\n",
    "\n",
    "The models were evaluated based on precision and recall due to the unbalanced nature of the dataset. Simply by predicting that all rows were non POIs roughly an ~85% accuracy could be obtained. However this would make the model completely ineffective of identifying POIs.\n",
    "\n",
    "**Due to random nature of testing actual values might be slightly different**\n",
    "\n",
    "| Model         | Precision     | Recall| F1 Score |\n",
    "| ------------- |:-------------:| ----- |--------- |\n",
    "| KMeans        | .176          |  .256 | .31      |\n",
    "| LinearSVC     | .24           |   .23 | .20      |\n",
    "| Logistic Reg  | .31           |   .35 | .33      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "Turning the parameters of the model essentially means tweaking the way the model classifies data into predictions. The models were initially tuned by testing various parameters specific to each model by using the GridSearchCV function of Sklearn. The models were ranked by their F1 score, which uses both recall and precision in it's calculation. Scoring was done by using StratifiedShuffleSplits with 1000 iterations on a train set. The best estimator was scored on a holdout test set.\n",
    "\n",
    "The three chosen models were more finely tuned using further GridSearchCV iterations, as well as hand tuning on the tester script. The parameter that had the largest effect was weighting the features for the Logistic Regression and LinearSVC.\n",
    "\n",
    "# Feature Selection\n",
    "Feature selection was done through PCA and univariate feature selection with the KBest method. The number of components, and k features were selected through GridSearch testing. Through testing the engineered features showed more value than the original feature, for instance the other_bool hold a higher score than the other col\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>14.510657</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>12.905193</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>8.500972</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_bool</th>\n",
       "      <td>8.130370</td>\n",
       "      <td>0.005309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses_bool</th>\n",
       "      <td>7.733091</td>\n",
       "      <td>0.006502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>6.178484</td>\n",
       "      <td>0.014624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus_bool</th>\n",
       "      <td>4.944041</td>\n",
       "      <td>0.028477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>4.048657</td>\n",
       "      <td>0.046951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>3.316696</td>\n",
       "      <td>0.071631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>2.895653</td>\n",
       "      <td>0.091989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>1.855142</td>\n",
       "      <td>0.176309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>1.832540</td>\n",
       "      <td>0.178942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>1.500994</td>\n",
       "      <td>0.223455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>1.225051</td>\n",
       "      <td>0.271081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>1.195019</td>\n",
       "      <td>0.276999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>0.864678</td>\n",
       "      <td>0.354718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>0.828166</td>\n",
       "      <td>0.365036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>0.298763</td>\n",
       "      <td>0.585901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>0.224509</td>\n",
       "      <td>0.636679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.213471</td>\n",
       "      <td>0.645085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>0.190325</td>\n",
       "      <td>0.663606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.997633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               score    pvalue\n",
       "exercised_stock_options    14.510657  0.000243\n",
       "total_stock_value          12.905193  0.000515\n",
       "salary                      8.500972  0.004400\n",
       "other_bool                  8.130370  0.005309\n",
       "expenses_bool               7.733091  0.006502\n",
       "deferred_income             6.178484  0.014624\n",
       "bonus_bool                  4.944041  0.028477\n",
       "bonus                       4.048657  0.046951\n",
       "expenses                    3.316696  0.071631\n",
       "long_term_incentive         2.895653  0.091989\n",
       "from_poi_to_this_person     1.855142  0.176309\n",
       "restricted_stock            1.832540  0.178942\n",
       "shared_receipt_with_poi     1.500994  0.223455\n",
       "deferral_payments           1.225051  0.271081\n",
       "director_fees               1.195019  0.276999\n",
       "restricted_stock_deferred   0.864678  0.354718\n",
       "from_messages               0.828166  0.365036\n",
       "from_this_person_to_poi     0.298763  0.585901\n",
       "loan_advances               0.224509  0.636679\n",
       "other                       0.213471  0.645085\n",
       "total_payments              0.190325  0.663606\n",
       "to_messages                 0.000009  0.997633"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_pickle(\"Kbestdf.p\").sort_values('score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Validation is the act of checking the model performance against a list of known results. The classic mistake is to fit and validate the model against the same dataset, thereby over predicting the model accuracy and fit. The strategy used in the analysis was two fold. For the initial fit the various models were validated using a holdout test dataset. For the final model the entire dataset was split using a Kfold split that was shuffled over a 1000 iterations to compile aggregate metrics for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "For this project precision and recall were used to determine the final efficiacy of the effort. Precision for the model is the Number of Correctly Identified POIs over the Total Positive Predictions. Recall is the Number of Correctly Identified POIs over the Total Number of POIs. Accuracy was not used as the dataset was very imbalanced. For a crime case, such as this one, a high recall would be desirable as it means we are correctly flagging all fraudulent inviduals for investigation, even if the overall precision of the model is low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful References\n",
    "* http://scikit-learn.org/stable/auto_examples/grid_search_digits.html\n",
    "* http://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
